{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %md\n",
    "### ingest circuits.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](data_ingestion.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerField, SringType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StructType represent the rows and StructField represents the columns\n",
    "\n",
    "circuits_schema = StructType(fields=[\n",
    "    StructField(\"circuitId\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lng\", DoubleType(), True),\n",
    "    StructField(\"alt\", IntegerType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 - read the csv file using the spark dataframe reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all the mounts point and source\n",
    "display(dbutils.fs.mounts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the path for a specific mount\n",
    "%fs\n",
    "ls /mnt/formula1dl/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_df = spark.read.csv(\"\")\n",
    "\n",
    "# this will display the headers in the columns\n",
    "circuits_df = spark.read \\\n",
    ".option(\"header\", True) \\\n",
    ".schema(circuits_schema) \\\n",
    ".csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will show the first 20 rows\n",
    "circuits_df_show()\n",
    "\n",
    "#this is better to show nicer data\n",
    "display(circuits_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK DATA TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use printSchema from df to check data type BECAREFUL DOUBLE CHECK IF IT IS ACCURATE\n",
    "circuits_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is more accurate for checking datatype\n",
    "# the min and max return will show if the datatype is right\n",
    "\n",
    "circuits_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are different ways to do this\n",
    "\n",
    "circuits_selected_df = circuits_df.select(\"circuitId\", \"circuitRef\", \"name\", \"location\", \"country\", \"lat\", \"lng\", \"alt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the columns\n",
    "\n",
    "circuits_selected_df = circuits_df.select(col(\"circuitId\").alias(\"circuit_id\"), col(\"circuitRef\").alias(\"circuit_ref\"), col(\"name\"), col(\"location\"), col(\"country\"),\n",
    "col(\"lat\"), col(\"lng\"), col(\"alt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename the columns as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_renamed_df = circuits_selected_df.withColumnRenamed(\"circuitId\", \"circuit_id\") \\\n",
    "    .withColumnRenamed(\"circuitRef\", \"circuit_ref\") \\\n",
    "    .withColumnRenamed(\"lat\", \"latitude\") \\\n",
    "    .withColumnRenamed(\"lng\", \"longitude\") \\\n",
    "    .withColumnRenamed(\"alt\", \"altitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ingestion date to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_final_df = circuits_renamed_df.withColumn(\"ingestion_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write data to datalake as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will save the data as a parquet file\n",
    "#the overwrite will replace the data if it already exists..there other options like display error, ignore and append\n",
    "#partiion helps break the files down into smaller files\n",
    "circuits_final_df_write.mode(\"overwrite\").paritionBy(\"location\").parquet(\"/mnt/formula1dl/processed/circuits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this to check the file is saved\n",
    "%fs\n",
    "ls /mnt/formula1dl/processed/circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/mnt/formula1dl/processed/circuits\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
